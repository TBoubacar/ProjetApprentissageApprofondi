{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5dd04a-67ec-456f-9f62-61c478018d92",
   "metadata": {},
   "source": [
    "# TP3 - Deep Learning : Méthodologie, Expérimentations et Régularisation \n",
    "\n",
    "<p style=\"font-weight: bold; font-size: 16px;\"><span style=\"color: red;\">Auteur : </span>TOURE Boubacar (<a href=\"boubacar.toure@etud.univ-angers.fr\">boubacar.toure@etud.univ-angers.fr</a>), étudiant en Master 2 Informatique à la faculté des sciences d'Angers</p>\n",
    "<p style=\"font-weight: bold; font-size: 16px;\"><span style=\"color: red;\">Professeur Référent : </span>Sylvain Lamprier (<a href=\"sylvain.lamprier@univ-angers.fr\">sylvain.lamprier@univ-angers.fr</a>)</p>\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (<a href=\"nicolas.baskiotis@sorbonne-univeriste.fr\">nicolas.baskiotis@sorbonne-univeriste.fr</a>) et Benjamin Piwowarski (<a href=\"benjamin.piwowarski@sorbonne-universite.fr\">benjamin.piwowarski@sorbonne-universite.fr</a>) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a31723-13bc-45bb-957c-203b416f2a13",
   "metadata": {},
   "source": [
    "# Import du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407ecdf-9996-405e-b0bf-6290b7517255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c7ed9-a8e6-4ae5-adb2-e503752475ee",
   "metadata": {},
   "source": [
    "# Déclaration des fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1171038-b872-4958-9dbe-b5cfc2ea4619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_state(fichier,epoch,model,optim):\n",
    "    state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "    torch.save(state,fichier)\n",
    "\n",
    "def load_state(fichier,model,optim):\n",
    "    epoch = 0\n",
    "    if os.path.isfile(fichier):\n",
    "        state = torch.load(fichier)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optim.load_state_dict(state['optim_state'])\n",
    "        epoch = state['epoch']\n",
    "    return epoch\n",
    "\n",
    "def unnormalize(img, mean, std):\n",
    "    if img.dim()==2 or ((img.dim()==3) and (img.size()[0]==1)):\n",
    "        return img*std[0]+mean[0]\n",
    "    return img * img.new(std).view(3, 1, 1) + img.new(mean).view(3, 1, 1)\n",
    "\n",
    "\n",
    "def train(model, train_loader, validation_loader, loss_fn, optimizer, epochs, typeTrain=\"\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    \n",
    "    # Clear any logs from previous runs\n",
    "    if os.path.exists(f\"/tmp/logs/deepLearning/{model.name}-train\"):\n",
    "        shutil.rmtree(f\"/tmp/logs/deepLearning/{model.name}-train\")\n",
    "    \n",
    "    # On créé un writer avec la date du modèle pour s'y retrouver\n",
    "    TB_PATH = f\"/tmp/logs/deepLearning\"\n",
    "    MODEL_PATH = \"/tmp/models/\"\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    check_file = f\"{MODEL_PATH}/{model.name}-train-{typeTrain}.pth\"\n",
    "    \n",
    "    summary = SummaryWriter(f\"{TB_PATH}/{model.name}-train\")\n",
    "    \n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    accuraciesOfValidation = []\n",
    "    accuraciesOfTrain = []\n",
    "\n",
    "    start_epoch = load_state(check_file, model, optimizer)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('\\n', '-' * 40)\n",
    "    for epoch in range(start_epoch+1, epochs+1):\n",
    "        # Entraînement sur les données d'entraînement\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_accuracy = correct / total\n",
    "        accuraciesOfTrain.append(train_accuracy)\n",
    "        summary.add_scalar(\"Accuracy_Of_Train\", train_accuracy, epoch)\n",
    "        summary.add_scalar(\"Loss_Of_Train\", train_loss/len(train_loader), epoch)\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "            \n",
    "        if epoch % 10 == 0: \n",
    "            # Evaluation sur les données de test\n",
    "            save_state(check_file, epoch, model, optimizer)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                validation_loss = 0.0\n",
    "                correct = 0.0\n",
    "                total = 0.0\n",
    "                for inputs, labels in validation_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    validation_loss += loss_fn(outputs, labels).item()\n",
    "            \n",
    "                validation_accuracy = correct / total\n",
    "                accuraciesOfValidation.append(validation_accuracy)\n",
    "                summary.add_scalar(\"Accuracy_Of_Validation\", validation_accuracy, epoch)\n",
    "                summary.add_scalar(\"Loss_Of_Validation\", validation_loss/len(validation_loader), epoch)\n",
    "                validation_losses.append(validation_loss/len(validation_loader))\n",
    "\n",
    "                print('\\t', '-' * 40)\n",
    "                print(\"\\tPredicted :\", predicted)\n",
    "                print(\"\\tResult :\", labels)\n",
    "                print(\"\\tTOTAL (correct/total):\", correct, '/', total)\n",
    "                print('\\t', '-' * 40)\n",
    "\n",
    "                print(\"\\tEpoch {}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f} %\".format(epoch, validation_loss, validation_accuracy * 100.0), '\\n')\n",
    "        else:\n",
    "            print(\"Epoch {}, Train Loss: {:.4f}, Train Accuracy: {:.4f} %\".format(epoch, train_loss/len(train_loader), train_accuracy * 100.0))\n",
    "    print('\\n', '-' * 40)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    trainTime = end_time - start_time\n",
    "    trainModelComplexity = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuraciesOfTrain)\n",
    "    plt.title('Train Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(validation_losses)\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuraciesOfValidation)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load the TensorBoard notebook extension\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {TB_PATH}/{model.name}-train\n",
    "    \n",
    "    return trainTime, trainModelComplexity, validation_loss, validation_accuracy\n",
    "\n",
    "def set_dropout_rate(m, rate):\n",
    "    if type(m) == nn.Dropout:\n",
    "        m.p = rate\n",
    "\n",
    "def train_dropout(model, train_loader, validation_loader, loss_fn, optimizer, epochs, dropout_rate, typeTrain=\"\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    \n",
    "    # Clear any logs from previous runs\n",
    "    if os.path.exists(f\"/tmp/logs/deepLearning/{model.name}-train_dropout\"):\n",
    "        shutil.rmtree(f\"/tmp/logs/deepLearning/{model.name}-train_dropout\")\n",
    "    \n",
    "    # On créé un writer avec la date du modèle pour s'y retrouver\n",
    "    TB_PATH = f\"/tmp/logs/deepLearning\"\n",
    "    MODEL_PATH = \"/tmp/models/\"\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    check_file = f\"{MODEL_PATH}/{model.name}-train_dropout-{typeTrain}.pth\"\n",
    "    \n",
    "    summary = SummaryWriter(f\"{TB_PATH}/{model.name}-train_dropout\")\n",
    "    \n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    accuraciesOfValidation = []\n",
    "    accuraciesOfTrain = []\n",
    "    \n",
    "    start_epoch = load_state(check_file, model, optimizer)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('\\n', '-' * 40)\n",
    "    for epoch in range(start_epoch+1, epochs+1):\n",
    "        # Entraînement sur les données d'entraînement\n",
    "        model.train()\n",
    "        model.apply(lambda x: set_dropout_rate(x, dropout_rate))\n",
    "        train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_accuracy = correct / total\n",
    "        accuraciesOfTrain.append(train_accuracy)\n",
    "        summary.add_scalar(\"Accuracy_Of_Train\", train_accuracy, epoch)\n",
    "        summary.add_scalar(\"Loss_Of_Train\", train_loss/len(train_loader), epoch)\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "            \n",
    "        if epoch % 10 == 0: \n",
    "            # Evaluation sur les données de test\n",
    "            model.eval()\n",
    "            model.apply(lambda x: set_dropout_rate(x, 0))\n",
    "            with torch.no_grad():\n",
    "                validation_loss = 0.0\n",
    "                correct = 0.0\n",
    "                total = 0.0\n",
    "                for inputs, labels in validation_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    validation_loss += loss_fn(outputs, labels).item()\n",
    "\n",
    "                validation_accuracy = correct / total\n",
    "                accuraciesOfValidation.append(validation_accuracy)\n",
    "                summary.add_scalar(\"Accuracy\", validation_accuracy, epoch)\n",
    "                summary.add_scalar(\"Loss_Of_Validation\", validation_loss/len(validation_loader), epoch)\n",
    "                validation_losses.append(validation_loss/len(validation_loader))\n",
    "                \n",
    "                print('\\t', '-' * 40)\n",
    "                print(\"\\tPredicted :\", predicted)\n",
    "                print(\"\\tResult :\", labels)\n",
    "                print(\"\\tTOTAL (correct/total):\", correct, '/', total)\n",
    "                print('\\t', '-' * 40)\n",
    "\n",
    "            \n",
    "                print(\"\\tEpoch {}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f} %\".format(epoch, validation_loss, validation_accuracy * 100.0, '\\n'))\n",
    "        else:\n",
    "            print(\"Epoch {}, Train Loss: {:.4f}, Train Accuracy: {:.4f} %\".format(epoch, train_loss/len(train_loader), train_accuracy * 100.0))\n",
    "    print('\\n', '-' * 40)\n",
    "    end_time = time.time()\n",
    "    trainTime = end_time - start_time\n",
    "    \n",
    "    trainModelComplexity = sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuraciesOfTrain)\n",
    "    plt.title('Train Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(validation_losses)\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuraciesOfValidation)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load the TensorBoard notebook extension\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {TB_PATH}/{model.name}-train_dropout\n",
    "    \n",
    "    return trainTime, trainModelComplexity, validation_loss, validation_accuracy\n",
    "\n",
    "def train_batchnorm(model, train_loader, validation_loader, loss_fn, optimizer, epochs, typeTrain=\"\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    \n",
    "    # Clear any logs from previous runs\n",
    "    if os.path.exists(f\"/tmp/logs/deepLearning/{model.name}-train_batchnorm\"):\n",
    "        shutil.rmtree(f\"/tmp/logs/deepLearning/{model.name}-train_batchnorm\")\n",
    "\n",
    "    # On créé un writer avec la date du modèle pour s'y retrouver\n",
    "    TB_PATH = f\"/tmp/logs/deepLearning\"\n",
    "    MODEL_PATH = \"/tmp/models/\"\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    check_file = f\"{MODEL_PATH}/{model.name}-train_batchnorm-{typeTrain}.pth\"\n",
    "\n",
    "    summary = SummaryWriter(f\"{TB_PATH}/{model.name}-train_batchnorm\")\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    accuraciesOfValidation = []\n",
    "    accuraciesOfTrain = []\n",
    "\n",
    "    start_epoch = load_state(check_file, model, optimizer)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('\\n', '-' * 40)\n",
    "    for epoch in range(start_epoch+1, epochs+1):\n",
    "        # Entraînement sur les données d'entraînement\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        accuraciesOfTrain.append(accuracy)\n",
    "        summary.add_scalar(\"Accuracy_Of_Train\", accuracy, epoch)\n",
    "        summary.add_scalar(\"Loss_Of_Train\", train_loss/len(train_loader), epoch)\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "\n",
    "        if epoch % 10 == 0: \n",
    "            # Evaluation sur les données de test\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                validation_loss = 0.0\n",
    "                correct = 0.0\n",
    "                total = 0.0\n",
    "                for inputs, labels in validation_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    validation_loss += loss_fn(outputs, labels).item()\n",
    "            \n",
    "                validation_accuracy = correct / total\n",
    "                accuraciesOfValidation.append(validation_accuracy)\n",
    "                summary.add_scalar(\"Accuracy\", validation_accuracy, epoch)\n",
    "                summary.add_scalar(\"Loss_Of_Validation\", validation_loss/len(validation_loader), epoch)\n",
    "                validation_losses.append(validation_loss/len(validation_loader))\n",
    "\n",
    "                print('\\t', '-' * 40)\n",
    "                print(\"\\tPredicted :\", predicted)\n",
    "                print(\"\\tResult :\", labels)\n",
    "                print(\"\\tTOTAL (correct/total):\", correct, '/', total)\n",
    "                print('\\t', '-' * 40)\n",
    "\n",
    "                print(\"\\tEpoch {}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f} %\".format(epoch, validation_loss, validation_accuracy * 100.0), '\\n')\n",
    "        else:\n",
    "            print(\"Epoch {}, Train Loss: {:.4f}, Train Accuracy: {:.4f} %\".format(epoch, train_loss/len(train_loader), train_accuracy * 100.0))\n",
    "    print('\\n', '-' * 40)\n",
    "    end_time = time.time()\n",
    "    trainTime = end_time - start_time\n",
    "    \n",
    "    trainModelComplexity = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuraciesOfTrain)\n",
    "    plt.title('Train Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(validation_losses)\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuraciesOfValidation)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load the TensorBoard notebook extension\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {TB_PATH}/{model.name}-train_batchnorm\n",
    "    \n",
    "    return trainTime, trainModelComplexity, validation_loss, validation_accuracy\n",
    "\n",
    "\n",
    "# requiert que les modules soient enregistrés dans une liste model.hidden_layers\n",
    "def addWeightsHisto(writer, model, epoch):                \n",
    "    ix = 0\n",
    "    for module in model.hidden_layers:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            writer.add_histogram(f'Linear/{ix}/weight', module.weight, epoch)\n",
    "            ix += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5c44f-3818-4c85-8c88-ac07df5d96a6",
   "metadata": {},
   "source": [
    "# Déclaration des class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f117d-587b-4d3f-afea-d25449911d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearMultiClass(nn.Module):\n",
    "    def __init__(self, in_size, out_size, hidden_layers, final_activation=None, activation=nn.Tanh()):\n",
    "        super(LinearMultiClass, self).__init__()\n",
    "        self.name = \"Linear_Multi_Class\"\n",
    "        self.in_size = in_size\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], out_size)\n",
    "        self.final_activation = final_activation\n",
    "        self.activation = activation\n",
    "        \n",
    "        for i, h in enumerate(hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(in_size if i == 0 else hidden_layers[i-1], h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.in_size)\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        if self.final_activation is not None:\n",
    "            x = self.final_activation(x)\n",
    "        return x\n",
    "\n",
    "class LinearMultiClassWithDropout(nn.Module):\n",
    "    def __init__(self, in_size, out_size, hidden_layers, dropout_rate=0.5, final_activation=None, activation=nn.Tanh()):\n",
    "        super(LinearMultiClassWithDropout, self).__init__()\n",
    "        self.name = \"Linear_Multi_Class_With_Dropout\"\n",
    "        self.in_size = in_size\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], out_size)\n",
    "        self.final_activation = final_activation\n",
    "        self.activation = activation\n",
    "        \n",
    "        for i, h in enumerate(hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(in_size if i == 0 else hidden_layers[i-1], h))\n",
    "            self.dropout_layers.append(nn.Dropout(p=self.dropout_rate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.in_size)\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout_layers[i](x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        if self.final_activation is not None:\n",
    "            x = self.final_activation(x)\n",
    "        return x\n",
    "\n",
    "class LinearMultiClassWithBatchNorm(nn.Module):\n",
    "    def __init__(self, in_size, out_size, hidden_layers, final_activation=None, activation=nn.Tanh()):\n",
    "        super(LinearMultiClassWithBatchNorm, self).__init__()\n",
    "        self.name = \"Linear_Multi_Class_With_BatchNorm\"\n",
    "        self.in_size = in_size\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.batch_norm_layers = nn.ModuleList()\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], out_size)\n",
    "        self.final_activation = final_activation\n",
    "        self.activation = activation\n",
    "        \n",
    "        for i, h in enumerate(hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(in_size if i == 0 else hidden_layers[i-1], h))\n",
    "            self.batch_norm_layers.append(nn.BatchNorm1d(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.in_size)\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            x = layer(x)\n",
    "            x = self.batch_norm_layers[i](x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        if self.final_activation is not None:\n",
    "            x = self.final_activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d130b0a-5a3c-47f9-ad4b-b2f61ded27be",
   "metadata": {},
   "source": [
    "# Déclaration des données du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a945583-a2cd-4cb6-b748-41f7bf49a44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Transformations à appliquer sur le dataset (transformation des images en tenseurs et normalization pour obtenir des valeurs entre -1 et 1)\n",
    "mean = [0.5]\n",
    "std = [0.5]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Téléchargement des données (via le dataset specifique MNIST de pytorch)\n",
    "mnist_train = MNIST('./data', train=True, transform=transform, download=True)\n",
    "mnist_test = MNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(mnist_test, batch_size=batch_size)\n",
    "\n",
    "TB_PATH = f\"/tmp/logs/deepLearning\"\n",
    "epoch = 100\n",
    "\n",
    "in_size = 784\n",
    "out_size = 10\n",
    "hidden_layers = [100, 100, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bb940-647b-4091-b68f-fd75bae6fd40",
   "metadata": {},
   "source": [
    "# Récuperation d'une image de notre batch de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b8512-a9a2-408e-aca2-ae67796c9fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs,labs = next(iter(train_loader))\n",
    "\n",
    "# dimension of images (flattened)reload_ext tensorboard\n",
    "HEIGHT,WIDTH = imgs.shape[2], imgs.shape[3] # taille de l'image\n",
    "INPUT_DIM = HEIGHT * WIDTH\n",
    "\n",
    "#Visualisation de la première image\n",
    "img = unnormalize(imgs[0], mean, std) # pour retrouver l'image d'origine (avant normalisation)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.imshow(img.squeeze(),cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebbe3da-66a4-45c8-8b14-3bdcc19d9be7",
   "metadata": {},
   "source": [
    "# Construction et Entrainement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f408405-1d30-415b-8ff1-d9faa8047265",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modèle LinearMultiClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978b1ca-dd14-4764-a841-a920026cc10e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Avec activation = nn.Tanh et Optimiseur = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ba835-b4cf-4324-bdf2-32bb19d44335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définir le modèle, la fonction de coût, et l'optimiseur\n",
    "model = LinearMultiClass(in_size=784, out_size=10, hidden_layers=[256, 128], activation=nn.Tanh())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr=1e-5)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model1ExecutionTimeTanhAdam, model1ComplexityTanhAdam, model1ValidationLossTanhAdam, model1AccuracyTanhAdam = train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"Tanh-Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75811b8d-8939-4876-b5a4-128f2a278074",
   "metadata": {},
   "source": [
    "### Avec activation = nn.Tanh et Optimiseur = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e76cf6-b48e-4fcd-ba9c-ded5f5be1a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définir le modèle, la fonction de coût, et l'optimiseur\n",
    "model = LinearMultiClass(in_size=784, out_size=10, hidden_layers=[256, 128], activation=nn.Tanh())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(),lr=1e-5)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model1ExecutionTimeTanhSGD, model1ComplexityTanhSGD, model1ValidationLossTanhSGD, model1AccuracyTanhSGD = train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"Tanh-SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c56513-3be8-40b9-8918-b1e7f980c1dd",
   "metadata": {},
   "source": [
    "### Avec activation = nn.ReLU et Optimiseur = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7de67d-a99e-470f-b292-c25177873984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définir le modèle, la fonction de coût, et l'optimiseur\n",
    "model = LinearMultiClass(in_size=784, out_size=10, hidden_layers=[256, 128], activation=nn.ReLU())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr=1e-5)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model1ExecutionTimeTanhReLuAdam, model1ComplexityTanhReLuAdam, model1ValidationLossTanhReLuAdam, model1AccuracyTanhReLuAdam = train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"ReLU-Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141a8a4-f2e1-4248-ad91-7e363662b772",
   "metadata": {},
   "source": [
    "### Avec activation = nn.ReLU et Optimiseur = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20877f1-4acc-4684-9ba1-57fecdd129da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définir le modèle, la fonction de coût, et l'optimiseur\n",
    "model = LinearMultiClass(in_size=784, out_size=10, hidden_layers=[256, 128], activation=nn.ReLU())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(),lr=1e-5)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model1ExecutionTimeTanhReLuSGD, model1ComplexityTanhReLuSGD, model1ValidationLossTanhReLuSGD, model1AccuracyTanhReLuSGD = train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"ReLU-SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b337529-9a39-44f7-84c1-663bd129362c",
   "metadata": {},
   "source": [
    "### Comparaison entre les activations (Tanh & ReLU) et les optimiseurs (Adam & SGD)\n",
    "\n",
    "La fonction d'activation Tanh (hyperbolique tangente) est une fonction de transfert sigmoid qui convertit une entrée linéaire en une sortie centrée autour de zéro dans l'intervalle [-1, 1]. Elle est souvent utilisée dans les réseaux de neurones de couches cachées pour normaliser les sorties des neurones.\n",
    "\n",
    "La fonction d'activation ReLU (rectifiée linéaire) est une fonction de transfert non linéaire qui convertit toutes les entrées négatives en zéro. Il est utilisé pour résoudre le problème de décès du neurone, qui est un phénomène où les poids des neurones peuvent devenir négatifs et rester coincés à zéro.\n",
    "\n",
    "En général, ReLU est considéré comme étant plus rapide et plus efficace pour les modèles de réseaux de neurones en raison de sa simplicité et de la vitesse de convergence plus rapide qu'il offre comparé à Tanh. Cependant, ReLU peut aussi causer des problèmes de saturation des neurones, où de nombreux neurones peuvent devenir saturés et ne pas produire de sortie. Cela peut rendre difficile la convergence du modèle. Tanh peut éviter ce problème mais il peut être plus lent à converger.\n",
    "\n",
    "En fin de compte, le choix de la fonction d'activation dépend du modèle et des données sur lesquels vous travaillez et des performances que vous recherchez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600a4f0-5bb7-4809-912c-e686cc55c2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Model Linear Multi Class With Tanh (Adam): [\\n\\tTemps d'exécution:\", model1ExecutionTimeTanhAdam,\"\\n\\tComplexité:\", model1ComplexityTanhAdam, \"\\n\\tValidation Loss:\", model1ValidationLossTanhAdam, \"\\n\\tAccurancy:\", model1AccuracyTanhAdam, \"] \\n\")\n",
    "print (\"Model Linear Multi Class With Tanh (SGD): [\\n\\tTemps d'exécution:\", model1ExecutionTimeTanhSGD,\"\\n\\tComplexité:\", model1ComplexityTanhSGD, \"\\n\\tValidation Loss:\", model1ValidationLossTanhSGD, \"\\n\\tAccurancy:\", model1AccuracyTanhSGD, \"] \\n\")\n",
    "\n",
    "if model1ValidationLossTanhAdam <= model1ValidationLossTanhSGD:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass avec Tanh utilisant l'optimiseur (Adam) est le plus performant !\\n___________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass avec Tanh utilisant l'optimiseur (SGD) est le plus performant !\\n___________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0b85d-c6e3-4d3a-8582-f5b7145ef97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Model Linear Multi Class With ReLU (Adam): [\\n\\tTemps d'exécution:\", model1ExecutionTimeTanhReLuAdam, \"\\n\\tComplexité:\", model1ComplexityTanhReLuAdam, \"\\n\\tValidation Loss:\", model1ValidationLossTanhReLuAdam, \"\\n\\tAccurancy:\", model1AccuracyTanhReLuAdam, \"] \\n\")\n",
    "print (\"Model Linear Multi Class With ReLU (SGD): [\\n\\tTemps d'exécution:\", model1ExecutionTimeTanhReLuSGD, \"\\n\\tComplexité:\", model1ComplexityTanhReLuSGD, \"\\n\\tValidation Loss:\", model1ValidationLossTanhReLuSGD, \"\\n\\tAccurancy:\", model1AccuracyTanhReLuSGD, \"] \\n\")\n",
    "\n",
    "if model1ValidationLossTanhReLuAdam <= model1ValidationLossTanhReLuSGD:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass avec ReLU utilisant l'optimiseur (Adam) est le plus performant !\\n___________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass avec ReLU utilisant l'optimiseur (SGD) est le plus performant !\\n___________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61dcfac-24b8-47a2-b4a8-c9e5ff4ce156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Model Linear Multi Class With Tanh (SGD): [\\n\\tTemps d'exécution:\", model1ExecutionTimeTanhSGD,\"\\n\\tComplexité:\", model1ComplexityTanhSGD, \"\\n\\tValidation Loss:\", model1ValidationLossTanhSGD, \"\\n\\tAccurancy:\", model1AccuracyTanhSGD, \"] \\n\")\n",
    "print (\"Model Linear Multi Class With ReLU (SGD): [\\n\\tTemps d'exécution:\", model1ExecutionTimeTanhReLuSGD, \"\\n\\tComplexité:\", model1ComplexityTanhReLuSGD, \"\\n\\tValidation Loss:\", model1ValidationLossTanhReLuSGD, \"\\n\\tAccurancy:\", model1AccuracyTanhReLuSGD, \"] \\n\")\n",
    "\n",
    "if model1ValidationLossTanhSGD <= model1ValidationLossTanhReLuSGD:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass avec Tanh utilisant l'optimiseur (SGD) est le plus performant !\\n___________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass avec ReLU utilisant l'optimiseur (SGD) est le plus performant !\\n___________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88441c-bc3e-483a-94c1-83f074e35293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Model Linear Multi Class With Tanh (Adam): [\\n\\tTemps d'exécution:\", model1ExecutionTimeTanhAdam,\"\\n\\tComplexité:\", model1ComplexityTanhAdam, \"\\n\\tValidation Loss:\", model1ValidationLossTanhAdam, \"\\n\\tAccurancy:\", model1AccuracyTanhAdam, \"] \\n\")\n",
    "print (\"Model Linear Multi Class With ReLU (Adam): [\\n\\tTemps d'exécution:\", model1ExecutionTimeTanhReLuAdam, \"\\n\\tComplexité:\", model1ComplexityTanhReLuAdam, \"\\n\\tValidation Loss:\", model1ValidationLossTanhReLuAdam, \"\\n\\tAccurancy:\", model1AccuracyTanhReLuAdam, \"] \\n\")\n",
    "\n",
    "if model1ValidationLossTanhAdam <= model1ValidationLossTanhReLuAdam:\n",
    "    print(\"Donc nous pouvons conclure que l'usage de la fonction d'entrainement en utilisant l'activation <Tanh> est bien plus efficace que l'activation <ReLU>. De plus l'optimiseur <Adam> est bien plus performant que l'optimiseur <SGD> car parmi les modèles testés :\")\n",
    "    print(\"___________________________________________________\\nLe Modèle LinearMultiClass avec Tanh utilisant l'optimiseur (Adam) est le plus performant !\\n___________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"Donc nous pouvons conclure que l'usage de la fonction d'entrainement en utilisant l'activation <ReLU> est bien plus efficace que l'activation <Tanh>. De plus l'optimiseur <Adam> est bien plus performant que l'optimiseur <SGD> car parmi les modèles testés :\")\n",
    "    print(\"___________________________________________________\\nLe Modèle LinearMultiClass avec ReLU utilisant l'optimiseur (Adam) est le plus performant !\\n___________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24620d1-00f9-4200-83fe-918c527c7278",
   "metadata": {},
   "source": [
    "### Construction d'un histogramme avec les données du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3caa1-29f6-47eb-b1e9-acb46d74b173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = SummaryWriter(f\"{TB_PATH}/{model.name}-historigramm\")\n",
    "addWeightsHisto(summary, model, epoch)\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {TB_PATH}/{model.name}-historigramm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626689fa-2e46-4c7f-aee2-00131df01c51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pénalisation des couches\n",
    "Une première technique pour éviter le sur-apprentissage est de régulariser chaque couche par une pénalisation sur les poids, i.e. de favoriser des poids faibles. On parle de pénalisation L1 lorsque la pénalité est de la forme $\\|W\\|_1$ et L2 lorsque la norme L2 est utilisée : $\\|W\\|_2^2$. En pratique, cela consiste à rajouter à la fonction de coût globale du réseau un terme en $\\lambda Pen(W)$ pour les paramètres de chaque couche que l'on veut régulariser.\n",
    "\n",
    "En résumé, l'utilisation de la régularisation des couches est importante pour aider à prévenir le sur-apprentissage et améliorer la performance générale du modèle sur des données non vues auparavant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccec895-965b-4be4-b31e-ef79652a04d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pour la norme L2 de 10^-5 avec l'optimiseur = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d18464-ccb6-4bef-b303-9e75a3a3603e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"__________________LA NORME L2 DE 10^−5 (Adam)__________________\")\n",
    "weight_decay = 10e-5\n",
    "model = LinearMultiClass(in_size, out_size, hidden_layers, activation=nn.Tanh())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"L2-Adam-10e-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3734e1c-652b-4d75-a647-33271b09c8f2",
   "metadata": {},
   "source": [
    "#### Pour la norme L2 de 10^-5 avec l'optimiseur = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0b526-b643-4b2c-a3e9-88df7d7a4a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"__________________LA NORME L2 DE 10^−5 (SGD)__________________\")\n",
    "weight_decay = 10e-5\n",
    "model = LinearMultiClass(in_size, out_size, hidden_layers, activation=nn.Tanh())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"L2-SGD-10e-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58584734-4919-4f69-95b8-5a8d0a2bdc65",
   "metadata": {},
   "source": [
    "#### Pour la norme L2 de 10^-4 avec l'optimiseur = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13065dc8-28bb-477a-9465-433c00bad0c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n__________________LA NORME L2 DE 10^-4 (Adam)__________________\")\n",
    "weight_decay = 10e-4\n",
    "model = LinearMultiClass(in_size, out_size, hidden_layers, activation=nn.Tanh())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"L2-Adam-10e-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8a68d-c139-4eca-98cc-8cb87d4a919f",
   "metadata": {},
   "source": [
    "#### Pour la norme L2 de 10^-4 avec l'optimiseur = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a489ed-3c00-4d75-988f-52e4eaed75c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n__________________LA NORME L2 DE 10^-4 (SGD)__________________\")\n",
    "weight_decay = 10e-4\n",
    "model = LinearMultiClass(in_size, out_size, hidden_layers, activation=nn.Tanh())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"L2-SGD-10e-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78763c7c-0419-426a-bb74-cafa71a85da5",
   "metadata": {},
   "source": [
    "#### Pour la norme L2 de 10^-3 avec l'optimiseur = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185a6b1-5396-4a3e-8d26-a61136f3f86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n__________________LA NORME L2 DE 10^-3 (Adam)__________________\")\n",
    "weight_decay = 10e-3\n",
    "model = LinearMultiClass(in_size, out_size, hidden_layers, activation=nn.Tanh())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"L2-Adam-10e-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7ad88-6837-44fc-94bc-22ebbf153489",
   "metadata": {},
   "source": [
    "#### Pour la norme L2 de 10^-3 avec l'optimiseur = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb32e0a-7aac-495c-8f25-d54192708c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n__________________LA NORME L2 DE 10^-3 (SGD)__________________\")\n",
    "weight_decay = 10e-3\n",
    "model = LinearMultiClass(in_size, out_size, hidden_layers, activation=nn.Tanh())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"L2-SGD-10e-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2a463-da10-4896-a861-ce94997a68d0",
   "metadata": {},
   "source": [
    "#### Pour la norme L2 de 0 avec l'optimiseur = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e549dd4-7f80-4d1a-ba49-e8b9f70fd3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n__________________LA NORME L2 DE 0 (Adam)__________________\")\n",
    "weight_decay = 0\n",
    "model = LinearMultiClass(in_size, out_size, hidden_layers, activation=nn.Tanh())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"L2-Adam-10e-0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51732e38-db3f-4289-9a1b-27192e3ee3ff",
   "metadata": {},
   "source": [
    "#### Pour la norme L2 de 0 avec l'optimiseur = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400ebce-2354-428c-82f0-97752623f0cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n__________________LA NORME L2 DE 0 (SGD)__________________\")\n",
    "weight_decay = 0\n",
    "model = LinearMultiClass(in_size, out_size, hidden_layers, activation=nn.Tanh())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "train(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"L2-SGD-10e-0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c29932-0ca6-419d-9736-658f6d54b4ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Conclusion :\n",
    "<p>Nous pouvons remarquer que lorsque nous utilisons l'optimiseur Adam, nous avons de nombreuses variations au niveau des courbes de pertes. Ce qui nous prouve que l'optimiseur SGD est le meilleur optimiseur lorsque nous appliquons la pénalisation des couches sur nos fonctions d'entrainement.</p>\n",
    "<p>De plus, on ne peut pas determiner quelle est la norme la mieux adaptée de manière générale car cela dépend de nombreux facteurs tels que la taille et la qualité des données d'entraînement, la structure et la complexité du modèle, etc.</p>\n",
    "<p>Mais en général, plus la valeur de la norme L2 est faible, plus le modèle sera pénalisé pour les poids importants. Ainsi, une valeur plus faible peut conduire à une régularisation plus forte et donc à un modèle plus simple et moins sujet au sur-apprentissage. Cependant, une régularisation trop forte peut entraîner un sous-apprentissage et une performance dégradée. D'un autre côté, une valeur plus élevée peut donner lieu à une régularisation plus faible, permettant ainsi au modèle de conserver davantage d'informations provenant des données d'entraînement. Cependant, une régularisation insuffisante peut entraîner un sur-apprentissage et une performance dégradée sur les données de validation ou de test.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77394b67-00e5-44e4-92f0-db66bc7be39b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modèle LinearMultiClassWithDropout\n",
    "\n",
    "Une autre technique très utilisée est le <a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html> **Dropout** </a>. L’idée du Dropout est proche du moyennage de modèle : en entraînant k modèles de manière indépendante, on réduit la variance du modèle. Entraîner k modèles présente un surcoût non négligeable, et l’intérêt du Dropout est de réduire la complexité mémoire/temps de calcul. Le Dropout consiste à chaque itération à *geler* certains neurones aléatoirement dans le réseau en fixant leur sortie à zéro. Cela a pour conséquence de rendre plus robuste le réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be72ff-7410-4edf-a6f4-c54d53d95175",
   "metadata": {},
   "source": [
    "### En utilisant l'optimiseur = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d278b6-d7c1-4cb0-955e-7f0b457dbb5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "model = LinearMultiClassWithDropout(in_size, out_size, hidden_layers, dropout_rate)\n",
    "\n",
    "# Define our loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model Dropout\n",
    "model2ExecutionTimeAdam, model2ComplexityAdam, model2ValidationLossAdam, model2AccuracyAdam = train_dropout(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, dropout_rate=dropout_rate, typeTrain=\"Dropout-Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b3572-4543-4a09-9aeb-4e742462a18b",
   "metadata": {},
   "source": [
    "### En utilisant l'optimiseur = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af10827-6de4-445d-a6b8-398bb2502311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_workersnum_workersdropout_rate = 0.5\n",
    "model = LinearMultiClassWithDropout(in_size, out_size, hidden_layers, dropout_rate)\n",
    "\n",
    "# Define our loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model Dropout\n",
    "model2ExecutionTimeSGD, model2ComplexitySGD, model2ValidationLossSGD, model2AccuracySGD = train_dropout(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, dropout_rate=dropout_rate, typeTrain=\"Dropout-SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ccada-85f8-44e9-9e7d-596b1b715c87",
   "metadata": {},
   "source": [
    "## Modèle LinearMultiClassWithBatchNorm\n",
    "\n",
    "On sait que les données centrées réduites permettent un apprentissage plus rapide et stable d’un modèle ; bien qu’on puisse faire en sorte que les données en entrées soient centrées réduites, cela est plus délicat pour les couches internes d’un réseau de neurones. La technique de <a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> **BatchNorm**</a> consiste à ajouter une couche qui a pour but de centrer/réduire les données en utilisant une moyenne/variance glissante (en inférence) et les statistiques du batch (en\n",
    "apprentissage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb21a0-aa94-436c-8bba-6677bc1965f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LinearMultiClassWithBatchNorm(in_size=784, out_size=10, hidden_layers=[256, 128, 64])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model3ExecutionTimeAdam, model3ComplexityAdam, model3ValidationLossAdam, model3AccuracyAdam = train_batchnorm(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"BatchNorm-Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c85e6-b193-43d8-97e0-89a116e5e674",
   "metadata": {},
   "source": [
    "### En utilisant l'activation = Softmax et l'optimiseur = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6a3be-1a20-4f29-af0f-aed859c0d7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LinearMultiClassWithBatchNorm(in_size=784, out_size=10, hidden_layers=[256, 128, 64], activation=nn.Softmax())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model3ExecutionTimeAdam, model3ComplexityAdam, model3ValidationLossAdam, model3AccuracyAdam = train_batchnorm(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"BatchNorm-Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a37394-6846-4a9f-a706-52d16ba6319a",
   "metadata": {},
   "source": [
    "### En utilisant l'activation = Softmax et l'optimiseur = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a40fbf-54c0-4714-af5e-4f9c2975fc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LinearMultiClassWithBatchNorm(in_size=784, out_size=10, hidden_layers=[256, 128, 64], activation=nn.Softmax())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "model3ExecutionTimeSGD, model3ComplexitySGD, model3ValidationLossSGD, model3AccuracySGD = train_batchnorm(model, train_loader, validation_loader, loss_fn, optimizer, epochs=100, typeTrain=\"BatchNorm-SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e970fd-6593-42b8-9937-294586026c74",
   "metadata": {},
   "source": [
    "# Fonction de comparaison des modèles \n",
    "\n",
    "Cette fonction renvoie maintenant des informations détaillées sur les deux modèles, y compris la perte, la précision, la complexité en termes de nombre de paramètres et le temps d'évaluation sur les données de validation. \n",
    "\n",
    "Pour comparer les performances d'un modèle sur des données de validation, il est plus pratique de comparer les moyennes des pertes de validation (validation_loss) calculées sur l'ensemble des données de validation pour chaque époque.\n",
    "\n",
    "<span style=\"color: red;\">N.B.</span> <span style=\"font-style: italic;\">L'accuracy est un métrique couramment utilisé pour évaluer la performance d'un modèle de classification. C'est la proportion de prédictions correctes que le modèle fait par rapport au nombre total de prédictions. L'accuracy est généralement exprimée en pourcentage et peut être calculée en comparant les étiquettes prévues par le modèle avec les étiquettes réelles. Plus l'accuracy est élevée, meilleure est la performance du modèle.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6570f29-dd45-4844-8670-8bf94713abe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_models(executionTimeOfModel1, complexityOfModel1, validationLossOfModel1, accuracyOfModel1, executionTimeOfModel2, complexityOfModel2, validationLossOfModel2, accuracyOfModel2):\n",
    "    print(\"Le modèle 1 a une perte de {} sur les données de validation avec une précision de {}% et une complexité de {} paramètres et un temps d'évaluation de {}s\".format(validationLossOfModel1, accuracyOfModel1*100, complexityOfModel1, executionTimeOfModel1))\n",
    "    print(\"\\nLe modèle 2 a une perte de {} sur les données de validation avec une précision de {}% et une complexité de {} paramètres et un temps d'évaluation de {}s\".format(validationLossOfModel2, accuracyOfModel2*100, complexityOfModel2, executionTimeOfModel2))\n",
    "\n",
    "    if validationLossOfModel1 < validationLossOfModel2:\n",
    "        print(\"\\n\\tLe modèle 1 a une perte plus faible sur les données de validation.\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"\\n\\tLe modèle 2 a une perte plus faible sur les données de validation.\")\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fa675-4e57-4c90-869e-5c445d9fd40f",
   "metadata": {},
   "source": [
    "# Comparaison des données des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38981fd7-fbec-4ce5-b6f2-b81bb02d020f",
   "metadata": {},
   "source": [
    "## Pour : LinearMultiClass VS LinearMultiClassWithDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25153925-8991-49d5-8a55-b7031a1b9472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultTest1 = compare_models(model1ExecutionTimeTanhAdam, model1ComplexityTanhAdam, model1ValidationLossTanhAdam, model1AccuracyTanhAdam, model2ExecutionTimeAdam, model2ComplexityAdam, model2ValidationLossAdam, model2AccuracyAdam)\n",
    "\n",
    "if resultTest1 == 1:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass (Adam) est le plus performant !\\n___________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClassWithDropout (Adam) est le plus performant !\\n___________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb21c5-efe5-4525-acca-b669595ace46",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTest1 = compare_models(model1ExecutionTimeTanhSGD, model1ComplexityTanhSGD, model1ValidationLossTanhSGD, model1AccuracyTanhSGD, model2ExecutionTimeSGD, model2ComplexitySGD, model2ValidationLossSGD, model2AccuracySGD)\n",
    "\n",
    "if resultTest1 == 1:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass (SGD) est le plus performant !\\n___________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClassWithDropout (SGD) est le plus performant !\\n___________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4392e38-c096-4d50-ada4-de705e18aec8",
   "metadata": {},
   "source": [
    "## Pour : LinearMultiClass VS LinearMultiClassWithBatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5104f4-e64b-4e1c-94ea-61930aec8f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultTest2 = compare_models(model1ExecutionTimeTanhAdam, model1ComplexityTanhAdam, model1ValidationLossTanhAdam, model1AccuracyTanhAdam, model3ExecutionTimeAdam, model3ComplexityAdam, model3ValidationLossAdam, model3AccuracyAdam)\n",
    "\n",
    "if resultTest2 == 1:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass (Adam) est le plus performant !\\n___________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClassWithBatchNorm (Adam) est le plus performant !\\n___________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b5d5f-8568-4e78-ab72-b77f87af08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTest2 = compare_models(model1ExecutionTimeTanhSGD, model1ComplexityTanhSGD, model1ValidationLossTanhSGD, model1AccuracyTanhSGD, model3ExecutionTimeSGD, model3ComplexitySGD, model3ValidationLossSGD, model3AccuracySGD)\n",
    "\n",
    "if resultTest2 == 1:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClass (SGD) est le plus performant !\\n___________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n___________________________________________________\\nLe Modèle LinearMultiClassWithBatchNorm (SGD) est le plus performant !\\n___________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b7f32-d47c-458c-874d-549340ac015a",
   "metadata": {},
   "source": [
    "## Pour : LinearMultiClassWithDropout VS LinearMultiClassWithBatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a991b17-cc71-4168-8d35-a8904b3274d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultTest3 = compare_models(model2ExecutionTimeAdam, model2ComplexityAdam, model2ValidationLossAdam, model2AccuracyAdam, model3ExecutionTimeAdam, model3ComplexityAdam, model3ValidationLossAdam, model3AccuracyAdam)\n",
    "\n",
    "if resultTest3 == 1:\n",
    "    print(\"\\n________________________________________________________________\\nLe Modèle LinearMultiClassWithDropout (Adam) est le plus performant !\\n________________________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n________________________________________________________________\\nLe Modèle LinearMultiClassWithBatchNorm (Adam) est le plus performant !\\n________________________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747bca1-1a81-47d7-b476-e3b0b7dd1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTest3 = compare_models(model2ExecutionTimeSGD, model2ComplexitySGD, model2ValidationLossSGD, model2AccuracySGD, model3ExecutionTimeSGD, model3ComplexitySGD, model3ValidationLossSGD, model3AccuracySGD)\n",
    "\n",
    "if resultTest3 == 1:\n",
    "    print(\"\\n________________________________________________________________\\nLe Modèle LinearMultiClassWithDropout (SGD) est le plus performant !\\n________________________________________________________________\\n\")\n",
    "else:\n",
    "    print(\"\\n________________________________________________________________\\nLe Modèle LinearMultiClassWithBatchNorm (SGD) est le plus performant !\\n________________________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7fa02-6c4b-4539-ae27-f8817de96b58",
   "metadata": {},
   "source": [
    "# Conclusion :\n",
    "\n",
    "En se basant sur les données recueillis lors de nos différents tests principalement en se basant le nombre de pertes de chaque modèle. <span style=\"font-style: italic;\"> Nous pouvons donc conclure que le modèle <span style=\"color:green;\">LinearMultiClass</span> est le plus performant des trois (3) modèles que nous avons eu à étudier avec nos jeux de données. En deuxième position <span style=\"color:yellow;\">LinearMultiClassWithBatchNorm</span> et enfin <span style=\"color:red;\">LinearMultiClassWithDropout</span></span> en utilisant l'optimiseur Adam. Par contre, en terme de précisions sur les données. Le modèle <span style=\"color:yellow;\">LinearMultiClassWithBatchNorm</span> est le meilleur parmi les trois modèles étudiés.\n",
    "\n",
    "Cependant, dans le cas où nous utilison l'optimiseur SGD. <span style=\"font-style: italic;\"> Nous pouvons remarquer que le modèle <span style=\"color:red;\">LinearMultiClassWithDropout</span> est le plus performant des trois (3) modèles que nous avons eu à étudier avec nos jeux de données. En deuxième position <span style=\"color:yellow;\">LinearMultiClassWithBatchNorm</span> et enfin <span style=\"color:green;\">LinearMultiClass</span></span>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
